<!doctype html><html lang=ja><head><title>[bpf-perf-workshop] lab1: レイテンシ調査 · /etc/openjny
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Junya Yamaguchi (@openjny)"><meta name=description content="Brendan Gregg 氏のハンズオン ワークショップ &ldquo;bpf-perf-workshop&rdquo; より、最初の課題 (アプリケーション レイテンシの調査課題) &ldquo;lab2&rdquo; をやってみました。

  課題
  
    
    見出しへのリンク
  
"><meta name=keywords content="blog,azure,cloud computing,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="[bpf-perf-workshop] lab1: レイテンシ調査"><meta name=twitter:description content="Brendan Gregg 氏のハンズオン ワークショップ “bpf-perf-workshop” より、最初の課題 (アプリケーション レイテンシの調査課題) “lab2” をやってみました。
課題 見出しへのリンク"><meta property="og:url" content="https://openjny.github.io/posts/2021/05/11/bpf-perf-workshop-lab1/"><meta property="og:site_name" content="/etc/openjny"><meta property="og:title" content="[bpf-perf-workshop] lab1: レイテンシ調査"><meta property="og:description" content="Brendan Gregg 氏のハンズオン ワークショップ “bpf-perf-workshop” より、最初の課題 (アプリケーション レイテンシの調査課題) “lab2” をやってみました。
課題 見出しへのリンク"><meta property="og:locale" content="ja"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-05-11T00:00:00+00:00"><meta property="article:modified_time" content="2021-05-11T00:00:00+00:00"><meta property="article:tag" content="Linux"><meta property="article:tag" content="Brendan Gregg"><meta property="article:tag" content="パフォーマンスチューニング"><meta property="article:tag" content="eBPF"><meta property="article:tag" content="Bcc"><link rel=canonical href=https://openjny.github.io/posts/2021/05/11/bpf-perf-workshop-lab1/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.140953f60ced76bc313e5d112c66fd4eb9df859f9cbc71a559db3d781dadcc70.css integrity="sha256-FAlT9gztdrwxPl0RLGb9TrnfhZ+cvHGlWds9eB2tzHA=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://openjny.github.io/>/etc/openjny
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>ブログ</a></li><li class=navigation-item><a class=navigation-link href=/tags/>タグ</a></li><li class=navigation-item><a class=navigation-link href=/categories/>カテゴリ</a></li><li class=navigation-item><a class=navigation-link href=/projects/>プロジェクト</a></li><li class=navigation-item><a class=navigation-link href=/about-me/>About Me</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/en/>🇺🇸</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://openjny.github.io/posts/2021/05/11/bpf-perf-workshop-lab1/>[bpf-perf-workshop] lab1: レイテンシ調査</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2021-05-11T00:00:00Z>2021年5月11日
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
10分で読めます</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/linux/>Linux</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/linux/>Linux</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/brendan-gregg/>Brendan Gregg</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/%E3%83%91%E3%83%95%E3%82%A9%E3%83%BC%E3%83%9E%E3%83%B3%E3%82%B9%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0/>パフォーマンスチューニング</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/ebpf/>eBPF</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/bcc/>Bcc</a></span></div></div></header><div class=post-content><p>Brendan Gregg 氏のハンズオン ワークショップ &ldquo;bpf-perf-workshop&rdquo; より、最初の課題 (アプリケーション レイテンシの調査課題) &ldquo;lab2&rdquo; をやってみました。</p><h2 id=課題>課題
<a class=heading-link href=#%e8%aa%b2%e9%a1%8c><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>今回の問題はこちら。</p><p><a href=https://github.com/brendangregg/bpf-perf-workshop/blob/master/lab1.md class=external-link target=_blank rel=noopener>bpf-perf-workshop/lab1.md at master · brendangregg/bpf-perf-workshop</a></p><blockquote><p>Problem Statement
An application has higher-than expected latency, including latency outliers. Why, and how can performance be improved?</p></blockquote><p>思ったようにパフォーマンスが出ない (i.e. 実行レイテンシが大きい) アプリケーション <code>./lab001</code> に対して、その理由と改善方法を調査しろ、という課題。</p><h2 id=60秒分析>60秒分析
<a class=heading-link href=#60%e7%a7%92%e5%88%86%e6%9e%90><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>まずは、有名な 60 秒分析で CPU、Memory、FileSystem/Disks、Network のいずれがボトルネックになっているかフィーリングを掴む。</p><p><a href=https://netflixtechblog.com/linux-performance-analysis-in-60-000-milliseconds-accc10403c55 class=external-link target=_blank rel=noopener>Linux Performance Analysis in 60,000 Milliseconds | by Netflix Technology Blog | Netflix TechBlog</a></p><pre tabindex=0><code>root@vm-ubuntu:~# uptime
 12:31:53 up 30 min,  3 users,  load average: 1.99, 1.37, 0.65
</code></pre><p><code>lab001</code> を実行した後の <code>uptime</code> の結果が上。後半 3 つの値は、それぞれ直近 1, 5, 15 分での load average (移動指数平均) を表している。</p><p>Linux での load average は、TASK_RUNNING + TASK_UNINTERRUPTIBLE 状態なタスク数の総和を表してることに注意すれば、<code>lab001</code> を実行したことでこれらタスクの数が増えてきていることが確認できる。</p><pre tabindex=0><code>root@vm-ubuntu:~# dmesg | tail
[   13.055942] audit: type=1400 audit(1620907273.224:11): apparmor=&#34;STATUS&#34; operation=&#34;profile_load&#34; profile=&#34;unconfined&#34; name=&#34;/usr/lib/snapd/snap-confine&#34; pid=841 comm=&#34;apparmor_parser&#34;
[   18.327127] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
[   23.839985]  sda: sda1
[   26.123281] EXT4-fs (sda1): mounted filesystem with ordered data mode. Opts: (null)
[   26.588156] new mount options do not match the existing superblock, will be ignored
[   57.834818] hv_balloon: Max. dynamic memory size: 16384 MB
[  109.372614] SGI XFS with ACLs, security attributes, realtime, no debug enabled
[  109.466644] JFS: nTxBlock = 8192, nTxLock = 65536
[  109.531712] ntfs: driver 2.1.32 [Flags: R/O MODULE].
[  109.685490] QNX4 filesystem 0.2.3 registered.
</code></pre><p><code>dmesg -T</code> とか見てみるとよく分かるけど、<code>lab001</code> を起動してからは特に異常が発生した様子はない。</p><pre tabindex=0><code>root@vm-ubuntu:~# vmstat -S M 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 1  1      0  14846     88    786    0    0    77   342   56  187  1  0 96  3  0
 0  1      0  14846     88    786    0    0     0  1696  556 1637  0  0 97  2  0
 0  1      0  14846     88    786    0    0     0  3712  372 1641  0  0 80 19  0
 0  1      0  14846     88    786    0    0     0  1776  355 1681  0  0 76 24  0
 0  1      0  14846     88    786    0    0     0  3788  540 1747  0  1 93  6  0
</code></pre><ul><li>UNINTERRUPTIBLE_SLEEP 状態なタスクが常に 1 つある</li><li>User (us), System (sy) 時間ともにほとんど消費していない</li><li>I/O 待ち (wa) 時間が比較的多い (24 %を占めるときもある)</li><li>Swap In/Out はゼロで問題なし</li><li>空きメモリも余裕がある</li></ul><p>以上の点から、CPU やメモリでのボトルネックではなく、I/O でのボトルネック説が疑われる。</p><pre tabindex=0><code>root@vm-ubuntu:~# mpstat -P ALL 1
Linux 5.4.0-1047-azure (vm-ubuntu)      05/13/21        _x86_64_        (4 CPU)

12:43:52     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:43:53     all    0.25    0.00    0.50   13.07    0.00    0.00    0.00    0.00    0.00   86.18
12:43:53       0    0.00    0.00    0.00   51.52    0.00    0.00    0.00    0.00    0.00   48.48
12:43:53       1    0.00    0.00    1.00    2.00    0.00    0.00    0.00    0.00    0.00   97.00
12:43:53       2    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
12:43:53       3    0.00    0.00    1.01    0.00    0.00    0.00    0.00    0.00    0.00   98.99

12:43:53     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:43:54     all    0.25    0.00    1.00    2.99    0.00    0.00    0.00    0.00    0.00   95.76
12:43:54       0    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
12:43:54       1    1.00    0.00    0.00    3.00    0.00    0.00    0.00    0.00    0.00   96.00
12:43:54       2    0.00    0.00    1.98    7.92    0.00    0.00    0.00    0.00    0.00   90.10
12:43:54       3    0.00    0.00    1.00    0.00    0.00    0.00    0.00    0.00    0.00   99.00

12:43:54     CPU    %usr   %nice    %sys %iowait    %irq   %soft  %steal  %guest  %gnice   %idle
12:43:55     all    0.00    0.00    0.50    1.50    0.00    0.00    0.00    0.00    0.00   97.99
12:43:55       0    0.00    0.00    0.00    0.00    0.00    1.00    0.00    0.00    0.00   99.00
12:43:55       1    0.00    0.00    1.00    3.00    0.00    0.00    0.00    0.00    0.00   96.00
12:43:55       2    0.00    0.00    1.00    4.00    0.00    0.00    0.00    0.00    0.00   95.00
12:43:55       3    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00  100.00
</code></pre><p>論理 CPU ごとに負荷がばらけているわけではなく、CPU#0 で I/O Wait が発生するタスクが実行されている模様。</p><pre tabindex=0><code>root@vm-ubuntu:~# pidstat 1
Linux 5.4.0-1047-azure (vm-ubuntu)      05/13/21        _x86_64_        (4 CPU)

12:45:12      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
12:45:13        0        11    0.99    0.00    0.00    0.00    0.99     3  rcu_sched
12:45:13        0       380    0.00    1.98    0.00    0.00    1.98     2  jbd2/sdb1-8
12:45:13        0      5689    0.00    0.99    0.00    0.00    0.99     1  lab001
12:45:13        0      7009    0.99    0.00    0.00    0.00    0.99     3  pidstat

12:45:13      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
12:45:14        0       380    0.00    1.00    0.00    0.00    1.00     2  jbd2/sdb1-8
12:45:14        0      5689    0.00    1.00    0.00    0.00    1.00     1  lab001
12:45:14        0      7009    0.00    1.00    0.00    0.00    1.00     3  pidstat

12:45:14      UID       PID    %usr %system  %guest   %wait    %CPU   CPU  Command
12:45:15        0       380    0.00    2.00    0.00    1.00    2.00     2  jbd2/sdb1-8
12:45:15        0      5689    0.00    1.00    0.00    0.00    1.00     1  lab001
</code></pre><p>pidstat が表示するパーセンテージは、全 CPU の総和である点に注意。あまり有益な情報は得られていない。</p><pre tabindex=0><code>root@vm-ubuntu:~# iostat -xz 1

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.25    0.00    0.00    1.76    0.00   97.99

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sdb              0.00  325.00      0.00   1748.00     0.00   112.00   0.00  25.63    0.00    3.00   0.68     0.00     5.38   3.06  99.60

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.00    0.00    0.25    1.51    0.00   98.24

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sdb              0.00  262.00      0.00   3424.00     0.00    86.00   0.00  24.71    0.00    3.79   0.80     0.00    13.07   3.83 100.40

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           0.00    0.00    0.25   12.56    0.00   87.19

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sdb              0.00  276.00      0.00   1472.00     0.00    92.00   0.00  25.00    0.00    3.59   0.78     0.00     5.33   3.64 100.40
</code></pre><p><code>/dev/sdb</code> に対して、Write 命令が多く見られる。具体的には、以下の通り:</p><ul><li>秒あたりの Read リクエスト完了数 (r/s) はゼロ</li><li>秒あたりの Write リクエスト完了数 (w/s) は 300 近い</li><li>秒あたりの Read セクタ量 (rkB/s) はゼロ</li><li>秒あたりの Write セクタ量 (wkB/s) は数千 Kbyte 近い</li></ul><p>また、Write 側のキューサイズ (wavgqu-sz) が恒常的に 1 を超えた値を記録しているので、物理デバイス I/O の飽和が発生していることが、ここからもわかる。</p><p>なお、Write リクエストのマージ率 (%wrqm) が 25% と比較的高いことから、同じようなセクタへの書き込みが多く発生していそうな匂いがする。</p><pre tabindex=0><code>root@vm-ubuntu:~# free -m
              total        used        free      shared  buff/cache   available
Mem:          16013         295       14840           0         877       15431
Swap:             0           0           0
</code></pre><p>free + buff/cache ないしは available に全然余裕があるので、問題なし。<code>dmseg</code> で <code>oom-killer</code> も観測していない。</p><pre tabindex=0><code>root@vm-ubuntu:~# sar -n DEV 1
Linux 5.4.0-1047-azure (vm-ubuntu)      05/13/21        _x86_64_        (4 CPU)

13:04:38        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
13:04:39           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
13:04:39         eth0      1.00      0.00      0.06      0.00      0.00      0.00      0.00      0.00

13:04:39        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
13:04:40           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
13:04:40         eth0      1.00      2.00      0.06      1.49      0.00      0.00      0.00      0.00

13:04:40        IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil
13:04:41           lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00
13:04:41         eth0      1.00      1.00      0.06      0.60      0.00      0.00      0.00      0.00
</code></pre><p>使用率 (%ifutil) を見ても全然余裕があるし、実際発生しているパケットの数や量 (*pck/s, *kB/s) も少ない。</p><pre tabindex=0><code>root@vm-ubuntu:~# sar -n TCP,ETCP 1
Linux 5.4.0-1047-azure (vm-ubuntu)      05/13/21        _x86_64_        (4 CPU)

13:07:05     active/s passive/s    iseg/s    oseg/s
13:07:06         0.00      0.00      1.00      0.00

13:07:05     atmptf/s  estres/s retrans/s isegerr/s   orsts/s
13:07:06         0.00      0.00      0.00      0.00      0.00

13:07:06     active/s passive/s    iseg/s    oseg/s
13:07:07         0.00      0.00      1.00      2.00

13:07:06     atmptf/s  estres/s retrans/s isegerr/s   orsts/s
13:07:07         0.00      0.00      0.00      0.00      0.00

13:07:07     active/s passive/s    iseg/s    oseg/s
13:07:08         0.00      0.00      1.00      2.00

13:07:07     atmptf/s  estres/s retrans/s isegerr/s   orsts/s
13:07:08         0.00      0.00      0.00      0.00      0.00
</code></pre><p>念の為みてみたが、TCP レベルの stats でも問題ない。</p><pre tabindex=0><code>root@vm-ubuntu:~# top

top - 13:08:08 up  1:07,  4 users,  load average: 2.05, 2.01, 1.90
Tasks: 157 total,   1 running,  89 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.2 us,  0.6 sy,  0.0 ni, 92.6 id,  6.5 wa,  0.0 hi,  0.2 si,  0.0 st
KiB Mem : 16398260 total, 15191744 free,   306500 used,   900016 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 15798152 avail Mem

  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
  380 root      20   0       0      0      0 D   1.3  0.0   0:33.53 jbd2/sdb1-8
 5689 root      20   0    6564   3016   1136 D   1.0  0.0   0:30.62 lab001
  418 root       0 -20       0      0      0 I   0.3  0.0   0:04.96 kworker/0:1H-kb
 8713 root      20   0   44560   3988   3344 R   0.3  0.0   0:00.05 top
    1 root      20   0   78000   9032   6572 S   0.0  0.1   0:02.95 systemd
</code></pre><p><code>D</code> (UNINTERRUPTIBLE_SLEEP) なプロセス 2 つが top の上位にきているので、こいつらが I/O (write) を占領していると考えるのが妥当そう。</p><h2 id=bcc-tools>bcc tools
<a class=heading-link href=#bcc-tools><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>上記の 60 秒 (自分がやると数分かかる) 分析によって、PID=5689 の <code>lab001</code> が　I/O Write を大量に発行しているために、システムに負荷がかかっている可能性が高いことがわかった。
続いて、bcc の一般的なツールを使って、更に詳細に分析していくことにする。</p><p><a href=https://github.com/iovisor/bcc/blob/master/docs/tutorial.md#1-general-performance class=external-link target=_blank rel=noopener>bcc/tutorial.md at master · iovisor/bcc</a></p><h3 id=execsnoop>execsnoop
<a class=heading-link href=#execsnoop><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./execsnoop.py
PCOMM            PID    PPID   RET ARGS
pgrep            9213   2672     0 /usr/bin/pgrep -U omsagent omiagent
sh               9214   2672     0 /bin/sh -c /opt/omi/bin/omicli wql root/scx &#34;SELECT PercentUserTime, PercentPrivilegedTime, UsedMemory, PercentUsedMemory FROM SCX_UnixProc
omicli           9216   9214     0 /opt/omi/bin/omicli wql root/scx SELECT PercentUserTime, PercentPrivilegedTime, UsedMemory, PercentUsedMemory FROM SCX_UnixProcessStatisticalInformation where Ha
grep             9217   9214     0 /bin/grep =
sh               9219   2672     0 /bin/sh -c /opt/omi/bin/omicli wql root/scx &#34;SELECT PercentUserTime, PercentPrivilegedTime, UsedMemory, PercentUsedMemory FROM SCX_UnixProc
</code></pre><p>特に 5689 から child process が生成されているわけではない。</p><h3 id=opensnoop>opensnoop
<a class=heading-link href=#opensnoop><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./opensnoop.py -p $(pgrep -nx lab001)
PID    COMM               FD ERR PATH
</code></pre><p>新たに file を open している訳ではない</p><h3 id=ext4slower-or-btrfs-xfs-zfs>ext4slower (or btrfs*, xfs*, zfs*)
<a class=heading-link href=#ext4slower-or-btrfs-xfs-zfs><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./ext4slower.py
Tracing ext4 operations slower than 10 ms
TIME     COMM           PID    T BYTES   OFF_KB   LAT(ms) FILENAME
13:22:38 lab001         5689   S 0       0          10.15 lab001.log
13:22:38 lab001         5689   S 0       0          11.70 lab001.log
13:22:39 lab001         5689   S 0       0          11.76 lab001.log
13:22:39 lab001         5689   S 0       0          10.42 lab001.log
13:22:39 lab001         5689   S 0       0          10.02 lab001.log
</code></pre><p>同じ path の file に対して、<code>S</code> の種別のオペレーションが多発している。<code>ext4slower.py</code> の実装を見ると、Type は以下のように定義されている。</p><ul><li><code>R</code>: read</li><li><code>W</code>: write</li><li><code>O</code>: open</li><li><code>S</code>: fsync</li></ul><p>つまり、1 秒間に数回の高頻度で fsync が実施されていることがわかる。</p><h3 id=biolatency>biolatency
<a class=heading-link href=#biolatency><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./biolatency.py -D -Q
Tracing block device I/O... Hit Ctrl-C to end.
^C

disk = b&#39;sdb&#39;
     usecs               : count     distribution
         0 -&gt; 1          : 0        |                                        |
         2 -&gt; 3          : 0        |                                        |
         4 -&gt; 7          : 0        |                                        |
         8 -&gt; 15         : 0        |                                        |
        16 -&gt; 31         : 0        |                                        |
        32 -&gt; 63         : 39       |****                                    |
        64 -&gt; 127        : 356      |****************************************|
       128 -&gt; 255        : 316      |***********************************     |
       256 -&gt; 511        : 169      |******************                      |
       512 -&gt; 1023       : 74       |********                                |
      1024 -&gt; 2047       : 16       |*                                       |
      2048 -&gt; 4095       : 4        |                                        |
      4096 -&gt; 8191       : 258      |****************************            |
      8192 -&gt; 16383      : 210      |***********************                 |
     16384 -&gt; 32767      : 12       |*                                       |
     32768 -&gt; 65535      : 1        |                                        |
</code></pre><p>Disk queue に入ってから完了するまでのレイテンシー分布を表示。分布が二峰になっているので、レイテンシーが遅いほうの分布は特定プログラム (i.e. lab001) による影響と仮設できる。</p><h3 id=biosnoop>biosnoop
<a class=heading-link href=#biosnoop><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./biosnoop.py
TIME(s)     COMM           PID    DISK    T SECTOR     BYTES  LAT(ms)
0.000000    jbd2/sdb1-8    380    sdb     W 2406576    4096      3.21
0.000348    lab001         5689   sdb     W 9009528    4096      0.21
0.000524    jbd2/sdb1-8    380    sdb     W 2406584    8192      0.14
0.004578    ?              0              R 0          0         4.00
0.007575    jbd2/sdb1-8    380    sdb     W 2406600    4096      2.96
0.007825    lab001         5689   sdb     W 9009528    4096      0.17
0.007917    jbd2/sdb1-8    380    sdb     W 2406608    8192      0.06
</code></pre><p>やはり lab001 による Write リクエストが多く発生している。ext4 用のジャーナリングシステムである <code>jdb2</code> が連動していること、lab001 が発行する wirte のサイズが 4096 であることを考慮して、以下のように推測できる:</p><ul><li>lab001 が頻繁に書き込みしているファイルシステムは ext4 であり、その block size はデフォルトの 4096 byte である。</li></ul><p>ファイルシステムに疎いので、また別の機会に ext4 と jdb2 について調べてみたい。</p><h3 id=cachestat>cachestat
<a class=heading-link href=#cachestat><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./cachestat.py
    HITS   MISSES  DIRTIES HITRATIO   BUFFERS_MB  CACHED_MB
       0        0      720    0.00%           93        782
     132        0      211  100.00%           93        782
       0        0      724    0.00%           93        782
     106        0      210  100.00%           93        782
       0        0      716    0.00%           93        782
     112        0      220  100.00%           93        782
       0        0      708    0.00%           93        782
</code></pre><p>ファイルシステムでのキャッシュヒット率には問題はなし。</p><h3 id=tcpconnect>tcpconnect
<a class=heading-link href=#tcpconnect><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./tcpconnect.py
Tracing connect ... Hit Ctrl-C to end
PID    COMM         IP SADDR            DADDR            DPORT
1628   python3      4  10.0.0.4         168.63.129.16    80
1628   python3      4  10.0.0.4         168.63.129.16    32526
1628   python3      4  10.0.0.4         168.63.129.16    80
1628   python3      4  10.0.0.4         169.254.169.254  80
1628   python3      4  10.0.0.4         168.63.129.16    80
</code></pre><p>Azure VM でホストした Ubuntu マシンだったので、管理用エージェント (waagent) が仮想 IP と通信している様子が見えた。それ以外に怪しい点はなし。</p><h3 id=tcpaccept>tcpaccept
<a class=heading-link href=#tcpaccept><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./tcpaccept.py
PID     COMM         IP RADDR            RPORT LADDR            LPORT
</code></pre><p>passive TCP コネクション (accept 経由の TCP コネクション確立) は特に発生なし。</p><h3 id=tcpretrans>tcpretrans
<a class=heading-link href=#tcpretrans><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./tcpretrans.py
Tracing retransmits ... Hit Ctrl-C to end
TIME     PID    IP LADDR:LPORT          T&gt; RADDR:RPORT          STATE
</code></pre><p>TCP コネクションの再送もなし。</p><h3 id=runqlat>runqlat
<a class=heading-link href=#runqlat><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./runqlat.py
Tracing run queue latency... Hit Ctrl-C to end.
^C
     usecs               : count     distribution
         0 -&gt; 1          : 200      |**                                      |
         2 -&gt; 3          : 779      |*********                               |
         4 -&gt; 7          : 1761     |********************                    |
         8 -&gt; 15         : 3433     |****************************************|
        16 -&gt; 31         : 1086     |************                            |
        32 -&gt; 63         : 91       |*                                       |
        64 -&gt; 127        : 649      |*******                                 |
       128 -&gt; 255        : 9        |                                        |
       256 -&gt; 511        : 3        |                                        |
       512 -&gt; 1023       : 3        |                                        |
      1024 -&gt; 2047       : 1        |                                        |
      2048 -&gt; 4095       : 1        |                                        |
</code></pre><p>ran queue レイテンシの分布を見ても、queue 内で長い間待たされている Task はほぼないので、CPU の飽和は認められない。</p><h2 id=答え合わせ>答え合わせ
<a class=heading-link href=#%e7%ad%94%e3%81%88%e5%90%88%e3%82%8f%e3%81%9b><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>実行していたアプリケーションのソースコードは下記に存在する。</p><p><a href=https://github.com/brendangregg/bpf-perf-workshop/blob/master/src/lab001.c class=external-link target=_blank rel=noopener>bpf-perf-workshop/lab001.c at master · brendangregg/bpf-perf-workshop</a></p><ul><li><code>fsync</code> を伴う <code>write</code> (os_write 関数) を永遠に回すプログラム</li><li>128 byte もしくは 2 Mbyte ずつ、zero 埋めされた配列を os_wirte で書き込んでいく。</li><li>ファイルサイズ (50 Mbyte) の最後まで zero 配列を書き込んだら、<code>lseek</code> で offset=0 に戻って再度やり直し。</li></ul><p>ということで、<code>fsync</code> を伴っているために、128 byte の書き込みを行うときは、ext4 の block size である 4096 byte に書き込みが expand される。そのため、殆どが無駄な I/O になってしまっていることがわかる。以下の <code>biosnoop.py</code> からも、4096 byte の物理 I/O (FIleSytem -> Disk の要求) が発生していることが確認できる。</p><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# ./biosnoop.py | grep lab001
TIME(s)     COMM           PID    DISK    T SECTOR     BYTES  LAT(ms)
...
2.197601    jbd2/sdb1-8    380    sdb     W 2384488    4096      3.45
2.197816    lab001         5689   sdb     W 8997120    4096      0.08
2.198063    jbd2/sdb1-8    380    sdb     W 2384496    8192      0.19
2.203421    ?              0              R 0          0         5.34
2.207513    jbd2/sdb1-8    380    sdb     W 2384512    4096      4.06
2.209320    lab001         5689   sdb     W 8997120    524288    0.47
2.209352    lab001         5689   sdb     W 8998144    524288    0.49
2.209510    lab001         5689   sdb     W 8999168    524288    0.64
2.209675    lab001         5689   sdb     W 9000192    524288    0.80
2.209854    lab001         5689   sdb     W 9001216    4096      0.97
2.209957    jbd2/sdb1-8    380    sdb     W 2384520    8192      0.06
2.229271    ?              0              R 0          0        19.30
2.232790    jbd2/sdb1-8    380    sdb     W 2384536    4096      3.48
2.233011    lab001         5689   sdb     W 9001216    4096      0.11
2.233180    jbd2/sdb1-8    380    sdb     W 2384544    8192      0.10
2.237200    ?              0              R 0          0         4.00
</code></pre><p>なお、途中で 524288 byte の write 要求 4 つ続く箇所があるが、これが 2 Mbyte の書き込みに相当する。</p><p>ext4 が物理 I/O を 524,288 byte に丸めこんでいるために 4 つに分割されていると推測できるが、今の知識では ext4 ファイルシステムが物理 I/O を計算する方法がわかららず、なぜ 4 分割されているのか (128 blocks 分で物理 I/O を発行しているのか) わからなかった。これは今後の課題としたい。</p><h3 id=対応策-fsync-をやめる>対応策: fsync をやめる
<a class=heading-link href=#%e5%af%be%e5%bf%9c%e7%ad%96-fsync-%e3%82%92%e3%82%84%e3%82%81%e3%82%8b><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>最も単純かつ効果の高い対応方法は <code>fsync</code> をやめることだろう。つまり、<code>lab001.c</code> に下記の変更を加える。</p><div class=highlight><pre tabindex=0 style=color:#abb2bf;background-color:#282c34;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-diff data-lang=diff><span style=display:flex><span><span style=color:#e06c75>- fsync(fd);
</span></span></span><span style=display:flex><span><span style=color:#e06c75></span><span style=color:#98c379;font-weight:700>+ //fsync(fd);
</span></span></span></code></pre></div><p>再コンパイルして得られた新たなアプリケーションでは、下記のように I/O の改善が見られた。
文章にすると、「file system の write buffer (キャッシュ) によって、物理 I/O の発行数が減り、<code>lab001</code> や ext4 ジャーナリングシステムが I/O 待ちすることがなくなった」というところでしょうか。</p><pre tabindex=0><code>root@vm-ubuntu:~/bcc/tools# uptime
 14:54:26 up  2:53,  5 users,  load average: 1.03, 1.23, 1.62

root@vm-ubuntu:~/bcc/tools# vmstat -S k 1
procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----
 r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st
 2  0      0 15414116 102080 949317    0    0    19   601  108  351  0  1 93  6  0
 1  0      0 15414108 102080 949317    0    0     0     0  100  261  2 23 75  0  0
 1  0      0 15414108 102080 949317    0    0     0     4  111  303  2 23 75  0  0
 1  0      0 15414108 102080 949317    0    0     0 96860   96  255  1 16 74 10  0
 1  0      0 15414206 102080 949317    0    0     0     0   54  243  2 24 75  0  0
 1  0      0 15414206 102080 949321    0    0     0     0   75  212  2 24 75  0  0
 1  0      0 15414206 102080 949321    0    0     0     0   68  175  1 24 75  0  0
 1  0      0 15414206 102080 949321    0    0     0     0   44  112  2 23 75  0  0
 1  0      0 15414206 102080 949321    0    0     0     4  219  462  2 24 74  0  0
 1  0      0 15414206 102080 949317    0    0     0     4  637 1297  2 24 74  0  0

root@vm-ubuntu:~/bcc/tools# iostat -xz 1
avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           2.26    0.00   23.06    0.00    0.00   74.69

Device            r/s     w/s     rkB/s     wkB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util
sdb              0.00    1.00      0.00      4.00     0.00     0.00   0.00   0.00    0.00    0.00   0.00     0.00     4.00   4.00   0.40
</code></pre><h3 id=その他の対応策>その他の対応策
<a class=heading-link href=#%e3%81%9d%e3%81%ae%e4%bb%96%e3%81%ae%e5%af%be%e5%bf%9c%e7%ad%96><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>おそらく他に効果のある対応は以下の通り (最後の方はシステムワイドな変更なので、多分実現可能性は低そう)。</p><ul><li>論理 I/O (write) の書き込みサイズを、統一して大きくする</li><li>同じ箇所 (論理 offset) の書き込みを繰り返すスレッドに分割する (e.g. thread0 は offset = [0, 4096] を担当)</li><li>ファイルシステム (ext4) のブロックサイズを、論理 I/O にあわせて調節する</li><li>ファイルシステムでジャーナリングを無効化する</li></ul></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2024 -
2025
Junya Yamaguchi (@openjny)
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "0a99e4ebb1e342beb03db419a6eea4a6"}'></script></body></html>