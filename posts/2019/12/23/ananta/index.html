<!doctype html><html lang=ja><head><title>Ananta: Azure を支えるステートフル L4 ロードバランサー · /etc/openjny
</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Junya Yamaguchi (@openjny)"><meta name=description content="この記事は Microsoft Azure Tech Advent Calendar 2019 の 18 日目の記事です。
今日は、Azure の裏側の話、データーセンター (DC) を支える、Ananta と呼ばれる Software Load Balancer (SLB) アーキテクチャを紹介したいと思います。

  はじめに
  
    
    見出しへのリンク
  
"><meta name=keywords content="blog,azure,cloud computing,personal"><meta name=twitter:card content="summary"><meta name=twitter:title content="Ananta: Azure を支えるステートフル L4 ロードバランサー"><meta name=twitter:description content="この記事は Microsoft Azure Tech Advent Calendar 2019 の 18 日目の記事です。 今日は、Azure の裏側の話、データーセンター (DC) を支える、Ananta と呼ばれる Software Load Balancer (SLB) アーキテクチャを紹介したいと思います。
はじめに 見出しへのリンク"><meta property="og:url" content="https://openjny.github.io/posts/2019/12/23/ananta/"><meta property="og:site_name" content="/etc/openjny"><meta property="og:title" content="Ananta: Azure を支えるステートフル L4 ロードバランサー"><meta property="og:description" content="この記事は Microsoft Azure Tech Advent Calendar 2019 の 18 日目の記事です。 今日は、Azure の裏側の話、データーセンター (DC) を支える、Ananta と呼ばれる Software Load Balancer (SLB) アーキテクチャを紹介したいと思います。
はじめに 見出しへのリンク"><meta property="og:locale" content="ja"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2019-12-23T00:00:00+00:00"><meta property="article:modified_time" content="2019-12-23T00:00:00+00:00"><meta property="article:tag" content="Azure"><meta property="article:tag" content="Load Balancer"><meta property="article:tag" content="Networking"><link rel=canonical href=https://openjny.github.io/posts/2019/12/23/ananta/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.140953f60ced76bc313e5d112c66fd4eb9df859f9cbc71a559db3d781dadcc70.css integrity="sha256-FAlT9gztdrwxPl0RLGb9TrnfhZ+cvHGlWds9eB2tzHA=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://openjny.github.io/>/etc/openjny
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/posts/>ブログ</a></li><li class=navigation-item><a class=navigation-link href=/tags/>タグ</a></li><li class=navigation-item><a class=navigation-link href=/categories/>カテゴリ</a></li><li class=navigation-item><a class=navigation-link href=/projects/>プロジェクト</a></li><li class=navigation-item><a class=navigation-link href=/about-me/>About Me</a></li><li class="navigation-item menu-separator"><span>|</span></li><li class=navigation-item><a href=/en/>🇺🇸</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://openjny.github.io/posts/2019/12/23/ananta/>Ananta: Azure を支えるステートフル L4 ロードバランサー</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa-solid fa-calendar" aria-hidden=true></i>
<time datetime=2019-12-23T00:00:00Z>2019年12月23日
</time></span><span class=reading-time><i class="fa-solid fa-clock" aria-hidden=true></i>
25分で読めます</span></div><div class=categories><i class="fa-solid fa-folder" aria-hidden=true></i>
<a href=/categories/azure/>Azure</a></div><div class=tags><i class="fa-solid fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/azure/>Azure</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/load-balancer/>Load Balancer</a>
</span><span class=separator>•</span>
<span class=tag><a href=/tags/networking/>Networking</a></span></div></div></header><div class=post-content><p>この記事は <a href=https://qiita.com/advent-calendar/2019/microsoft-azure-tech class=external-link target=_blank rel=noopener>Microsoft Azure Tech Advent Calendar 2019</a> の 18 日目の記事です。
今日は、Azure の裏側の話、データーセンター (DC) を支える、Ananta と呼ばれる Software Load Balancer (SLB) アーキテクチャを紹介したいと思います。</p><h2 id=はじめに>はじめに
<a class=heading-link href=#%e3%81%af%e3%81%98%e3%82%81%e3%81%ab><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>紹介の前に、まず最初に断っておかなければならないのは、<strong>Ananta が提案されたのは 2013 年であり、今から 6 年以上も前である</strong>ということです<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>。当然、現在の Azure の DC で動いているロードバランサー (LB) は Ananta ではなく、以下のような多くの技術<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>を含む、改良を加えたバージョンのものです 。</p><ul><li><strong>Duet [R. Gandhi, et al. 2014]</strong> <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> : Ananta の論文が公開された翌年 2014 年に提案。SLB である Ananta の一部を、汎用的なスイッチによってハードウェア処理とすることで、低レイテンシー・高帯域を実現。</li><li><strong>Rubik [R. Gandhi, et al. 2015]</strong> <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup> : Duet のメリットを残したまま、DC 内のリンクの利用効率を 4 倍も向上し、結果としてネットワーク帯域量を大幅に削減した。</li><li><strong>VFP [D. Firestone 2017]</strong> <sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> : ホスト OS の NIC で行っていた VIP/DIP 変換や SLB 関連の処理を、コントロール/データ プレーンに分離することでスケールを可能とした。OpenFlow (Open vSwitch) にインスパイアされた MAT モデルに基づき、Hypter-V の仮想スイッチの拡張機能として実装される。</li><li><strong>SmartNIC [D. Firestone, et al. 2018]</strong> <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> : NIC の帯域が増えるにつれボトルネックとなっていた、CPU による VFP 処理の一部をハードウェア処理に置換。ASIC の融通の効かなさをカバーする為に FPGA を活用 <sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>。</li></ul><ul><li><a href=https://blog.acolyer.org/2018/05/01/azure-accelerated-networking-smartnics-in-the-public-cloud/ class=external-link target=_blank rel=noopener>Azure accelerated networking: SmartNICs in the public cloud – the morning paper</a></li><li><a href=https://www.usenix.org/sites/default/files/conference/protected-files/nsdi18_slides_firestone.pdf class=external-link target=_blank rel=noopener>paper slide</a></li><li><a href=https://ascii.jp/elem/000/001/459/1459849/ class=external-link target=_blank rel=noopener>ASCII.jp：自社開発技術満載！Microsoft Azureの物理インフラを大解剖 (1/3)</a></li><li><a href=https://www.slideshare.net/ToruMakabe/azure-78491509 class=external-link target=_blank rel=noopener>インフラ野郎 Azureチーム 博多夏祭り</a> p.56</li></ul><h3 id=ananta-を学ぶメリット>Ananta を学ぶメリット
<a class=heading-link href=#ananta-%e3%82%92%e5%ad%a6%e3%81%b6%e3%83%a1%e3%83%aa%e3%83%83%e3%83%88><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>じゃあ、なぜ 2020 年にもなろうというのに今更 Ananta を学ぶべきなの？という話なんですが、個人的には、Ananta の仕組みを知るメリットは大きく 4 つあると思っています。</p><h4 id=1-azure-を使う時に-azure-の気持ちになれるから>1. Azure を使う時に Azure の気持ちになれるから
<a class=heading-link href=#1-azure-%e3%82%92%e4%bd%bf%e3%81%86%e6%99%82%e3%81%ab-azure-%e3%81%ae%e6%b0%97%e6%8c%81%e3%81%a1%e3%81%ab%e3%81%aa%e3%82%8c%e3%82%8b%e3%81%8b%e3%82%89><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>これはユーザー目線のメリットです。</p><p>Ananta は Azure の DC アーキテクチャの中心と言っても過言ではありません。というのも、Ananta に基づく SLB のアーキテクチャを利用している限り、データ プレーン (ネットワーク・トポロジー) はほぼほぼ固定されるからです。トポロジーが決まるということは、トラフィックのフローが決まるということですから、<strong>インターネットからやってきたトラフィックがどのように VM に流れ込むのか</strong> 理解することに繋がります。</p><p>さらに、Azure の (主にネットワーク関連の) 仕様が「アーキテクチャから自然と導かれる」ことを理解できれば、<strong>所謂 Azure でのお作法を丸暗記する必要もある程度なくなります</strong>。</p><p>例えば、記事のタイトルにもあるように、Ananta は<strong>ステートフル</strong> な <strong>L4</strong> ロード バランサーを <strong>ソフトウェア実装</strong> で実現するものです。ちょっと Azure の動作に関して踏み込んだ内容にはなりますが、この点を意識するだけでも以下のお作法を理解する助けになります。</p><ul><li>ステートフル<ul><li>Azure Load Balancer のバックエンド プールから実サーバー (VM) を切り離しても、確立している既存の TCP コネクションに関しては依然 VM に流れます。</li></ul></li><li>L4<ul><li>Azure Load Balancer では TCP/UDP のレイヤーまでしか考慮しないので、TLS 終端ができません (L7 ロードバランサーとしては、Application Gateway を使う必要があります)。また、HTTP のエラー (e.g. 403, 503 の HTTP status code) を Azure Load Balancer が吐くことはありません。</li></ul></li><li>ソフトウェア実装<ul><li>SLB が DC 内部に渡って極度に分散されている仕組みを知ることで、<strong>Azure Load Balancer がダウンすることと、リージョン全体がダウンする障害がほぼ等価である</strong>こともわかります。<a href=https://blogs.technet.microsoft.com/jpaztech/2017/02/07/azure-lb-pre-warming-and-monitoring/ class=external-link target=_blank rel=noopener>この公式ブログの記事</a> や<a href="https://www.syuheiuda.com/?p=4875" class=external-link target=_blank rel=noopener>宇田さんの記事</a>にも書かれてあるように、こうした知識を知っていれば、トラブルシュートの際のアクションの順番を決定することにも役立ちます。</li></ul></li></ul><h4 id=2-microsoft-の-sdn-技術を学ぶ土台になるから>2. Microsoft の SDN 技術を学ぶ土台になるから
<a class=heading-link href=#2-microsoft-%e3%81%ae-sdn-%e6%8a%80%e8%a1%93%e3%82%92%e5%ad%a6%e3%81%b6%e5%9c%9f%e5%8f%b0%e3%81%ab%e3%81%aa%e3%82%8b%e3%81%8b%e3%82%89><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>これは、Azure (Microsoft) でのデータセンター・インフラストラクチャ/ネットワーク技術を知りたい人向けのメリットです。</p><p>わかりやすい例で言えば、冒頭に示した Duet, VFP, SmartNIC など、<strong>Ananta の知識無しに学ぼうと思っても、正しく理解するのが難しい Microsoft の技術が多く存在します</strong>。他の例としては、Windows Server 2019 で実装されている SDN 技術 (e.g. SLB) も、詳細は公開されていませんが、アーキテクチャを見る限り Ananta (および VFP) がベースであるのは間違いないでしょう。</p><p>このように、Microsoft Research が発表しているネットワーク関係の論文<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>や技術には Ananta をベースとしているものが多く存在し、Microsoft として Ananta ライクな SLB に力を入れているのが分かります。<strong>今後 Microsoft が公開するであろうネットワーク技術をキャッチアップしていく土台を作る</strong>目的で Ananta を学ぶのは良い初手と言えます。</p><h4 id=3-slb-のリファレンス-アーキテクチャとなるから>3. SLB のリファレンス アーキテクチャとなるから
<a class=heading-link href=#3-slb-%e3%81%ae%e3%83%aa%e3%83%95%e3%82%a1%e3%83%ac%e3%83%b3%e3%82%b9-%e3%82%a2%e3%83%bc%e3%82%ad%e3%83%86%e3%82%af%e3%83%81%e3%83%a3%e3%81%a8%e3%81%aa%e3%82%8b%e3%81%8b%e3%82%89><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>SLB 技術に興味のある人へのメリットです。</p><p>Ananta 以外の データ センター向け L4 ロードバランサー としては、Facebook/Google の DC で動く Load Balancer が、それぞれ &ldquo;SlikRoad&rdquo;, &ldquo;Maglev&rdquo; という名前で論文が公開されています。そして、驚くことに、両論文ともに Ananta を Related Wroks として引用しており、提案手法との違いを議論しています。</p><ul><li><a href=https://research.fb.com/publications/silkroad-making-stateful-layer-4-load-balancing-fast-and-cheap-using-switching-asics/ class=external-link target=_blank rel=noopener>SilkRoad: Making Stateful Layer-4 Load Balancing Fast and Cheap Using Switching ASICs - Facebook Research</a></li><li><a href=https://www.usenix.org/conference/nsdi16/technical-sessions/presentation/eisenbud class=external-link target=_blank rel=noopener>Maglev: A Fast and Reliable Software Network Load Balancer | USENIX</a></li></ul><p>自分が Ananta の論文を読んだ時、Ananta は &ldquo;off-the-shelf&rdquo; な技術を活用しているので、本当の真新しさというのは一見するとわかりませんでした。しかし、その反面、<strong>アーキテクチャとしての美しさ</strong>を感じました。</p><p>ここからはネットワーク素人の個人的な見解となってしまうのですが、上の論文を見ると、(細かい部分に違いはあるもののの) ベースになっているコントロール/データ プレーンの考え方は Ananta に似ています。Ananta が分散 SLB としてよいリファレンス アーキテクチャであるために、SilkRoad や Maglev でも参考にされているのではないかと思っています。</p><h4 id=4-知らないことを知るのは楽しいから>4. 知らないことを知るのは楽しいから
<a class=heading-link href=#4-%e7%9f%a5%e3%82%89%e3%81%aa%e3%81%84%e3%81%93%e3%81%a8%e3%82%92%e7%9f%a5%e3%82%8b%e3%81%ae%e3%81%af%e6%a5%bd%e3%81%97%e3%81%84%e3%81%8b%e3%82%89><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>結局、これに尽きるんですよね。
仰々しくメリットを書いてみましたが、自分にとっては好奇心が一番大きなモチベーションです😝</p><h2 id=背景>背景
<a class=heading-link href=#%e8%83%8c%e6%99%af><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>このセクションでは、<strong>Ananta が提供する機能</strong> を背景を踏まえた上で説明します。</p><h3 id=サービスvipdip>サービス/VIP/DIP
<a class=heading-link href=#%e3%82%b5%e3%83%bc%e3%83%93%e3%82%b9vipdip><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/254f3e39-b3b2-aedd-552b-2db3374747d0.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h4 id=サービス>サービス
<a class=heading-link href=#%e3%82%b5%e3%83%bc%e3%83%93%e3%82%b9><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>IT の世界では、人間/機械/システムにとってある程度意味をなす機能のまとまり (高レベルな概念での機能の単位) を**サービス (service)**と呼びます<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup>。サービスは、一般に複数のコンピュータによって提供され、Web アプリケーション、VPN、データベースなど、多種多様です。例えば、ブラウザで YouTube にアクセスし、お気に入りのうさぎ🐰の動画が視聴できるのも、Google が youtube.com というエンドポイントで「動画サービス」を提供しているからです。</p><p>データ センターは多くのサービスをホストする場所です。また、Azure は、IaaS/PaaS という方法を通して多くの Azure ユーザーが自由にサービスをホスティングできる機能を提供しており、さらに Microsoft 自身も自社サービス (e.g. PaaS, Office 365, Dynamics 365) を Azure のデータ センターにデプロイしています。</p><p>このように、DC 内にサービスをホストする主体 (テナント) が複数存在することを、<strong>マルチテナント (multi-tenant)</strong> あるいは <strong>マルチテナンシー (multitenancy)</strong> といいます。一般に、サービスが違えばテナントも異なります (もちろん実際には、異なる二つのサービスを、同じテナントが所有するケースもかなり多いです)。</p><h4 id=vipdip>VIP/DIP
<a class=heading-link href=#vipdip><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>サービスの定義は状況に応じて変化する<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup>ため曖昧ですが、LB の文脈でのサービスは「外部からサービスに アクセスできる 1 つの IP アドレスが関連付くもの」と定義されます。そして、その IP アドレスを <strong>仮想 IP (virtual IP; VIP)</strong> といいます<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup>。</p><p><strong>一般的には VIP はグローバル IP アドレスとなるケースが多いです</strong>。例えば、WEB サイトの URL を名前解決したら出てくる IP アドレスは VIP です。</p><p>そして、VIP と対比する形で、サービス内部でのみ有効な IP アドレスを <strong>Direct IP (DIP)</strong> と呼びます。高可用性のサービスを提供するために、サービス内部では一般に複数のサーバー (バックエンド サーバー) <sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup>が動いており、各サーバーにはそれぞれ一つの DIP が付与されます。</p><p>誤解を恐れずに言えば、VIP = グローバル IP アドレス、DIP = プライベート IP アドレスだと思ってください。</p><h3 id=azure>Azure
<a class=heading-link href=#azure><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>Azure は Microsoft の提供するパブリック クラウド サービスです。
54 のリージョンと、140 カ国以上の国で利用が可能、さらにリージョン内の帯域幅は最大で 1.6 Pbps ！、なんてのが売りの一つです。<figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/1207499f-4f85-4a75-e8df-30f97f36bb84.png alt=image.png><figcaption><p>image.png</p></figcaption></figure></p><p><em>Azure は雲ではありません</em><sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup> ので裏側では当然物理サーバーが動いているわけですが、クラウド サービスのデーターセンター (DC) と聞くと、「なんかトラフィック多そう🤔」「トラフィック捌くの大変そう…」のような漠然とした印象を受けるのではないでしょうか？</p><p>少なくとも、(ネットワークの勉強を始める) 半年前の自分はそう思っていました・・・。</p><h4 id=dc-の抱える問題>DC の抱える問題
<a class=heading-link href=#dc-%e3%81%ae%e6%8a%b1%e3%81%88%e3%82%8b%e5%95%8f%e9%a1%8c><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>実際、Azure の DC にやってくるトラフィックをさばくのは大変です。</p><p>ただ、苦労しているのは Azure に限りません。これは、パブリック クラウドをホストする GCP, AWS はもちろん、世界中の DC での課題です。</p><p>例えば、Facebook 社は、<a href=https://engineering.fb.com/production-engineering/introducing-data-center-fabric-the-next-generation-facebook-data-center-network/ class=external-link target=_blank rel=noopener>2014 年のブログ記事</a>で自社サービスをホストする DC のネットワーク事情を語っていますが、サービス同士の「連携」が増えることによって、DC 内部のサーバー間通信量は <strong>毎年 2 倍のペースで指数関数的増加している傾向がある</strong> と報告しています。(マイクロサービスアーキテクチャの影響？)</p><blockquote><p>What happens inside the Facebook data centers – “machine to machine” traffic – is several orders of magnitude larger than what goes out to the Internet.
(中略)
We are constantly optimizing internal application efficiency, but nonetheless the rate of our machine-to-machine traffic growth remains exponential, and the volume has been doubling at an interval of less than a year.</p></blockquote><p>Google のデータセンターでも、2008 年から 2014 年にかけて<strong>50 倍ものトラフィック増加</strong>を観測していて、概ね年に 2 倍のペースという点で Facebook の状況と類似しています<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup>。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/bb16e7bf-51c0-19a1-db14-9f1e4b003f84.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h4 id=azure-の-dc-への要請>Azure の DC への要請
<a class=heading-link href=#azure-%e3%81%ae-dc-%e3%81%b8%e3%81%ae%e8%a6%81%e8%ab%8b><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>2013 年当時、Azure も引けず劣らず大変な状況でした。ユーザー数や計算機資源の投資額の数字としても、<strong>尋常じゃない量のトラフィックを捌く能力が必要</strong>とされていた (今もですが) ことがわかります。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/240c4f99-381f-ab6c-c332-59101875e22d.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><p>また、パブリック クラウドの DC では、ストレージ、コンピューティング、あるいは Web/データ分析等を目的とした PaaS といった、<strong>多種多様なサービスをユーザーに提供</strong>できる必要があります。ビジネス的な要請として、ユーザー自身も IaaS/PaaS とよばれるサービス形態を通して自由にサービスを DC 内にデプロイできます。しかも、Azure では、仮想ネットワーク」という名前で知られる ”論理的なプライベート IP アドレス空間” 機能を提供しています。これまたネットワーク的には難しい話です。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/b9427895-a6e1-b170-ddef-fe3b04f810af.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><p>あとは、仮想化技術で Azure は支えられているということも忘れてはいけません。いままでの図では、物理的なことは一切書いていませんでしたが、実際は、サービスの VM はこんな感じでデーターセンター内のいろんな場所にサービス内部の VM が分散配置されています (かなり概略)。
物理的な計算機資源を最大限に利活用するためには、適切なリソースのアロケーション不可欠なので、<strong>サービスをホストする物理的な場所がころころ変わります</strong> <sup id=fnref:15><a href=#fn:15 class=footnote-ref role=doc-noteref>15</a></sup>。</p><h3 id=負荷分散と-nat>負荷分散と NAT
<a class=heading-link href=#%e8%b2%a0%e8%8d%b7%e5%88%86%e6%95%a3%e3%81%a8-nat><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>さて、概念図に戻って、サービスの通信を DC としてどのように捌く必要があるのか見てみましょう。</p><p>実際にトラフィックを捌く仕組みは一旦置いといて、ここでは「なにかのしくみ」としておきます (後に見るように「なにかのしくみ」の新しい提案が Ananta であり、主に MUX とホスト エージェントによって実装されます)。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/f792ce16-22d5-235f-22c7-8e1ed16cde2a.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h4 id=inbound-通信>Inbound 通信
<a class=heading-link href=#inbound-%e9%80%9a%e4%bf%a1><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>図の左側の話です。</p><p>任意の場所のクライアントから、サービスの VIP (e.g. <code>VIP1</code>) にアクセスしたときを考えましょう。例えば図の青色のサービスでは、バックエンド サーバーは 2 台なので、「なにかのしくみ」を通してどちらの VM にパケットを渡すか決めてあげる必要があります。これを <strong>負荷分散 (load balancing、ロード バランシング)</strong> と言います <sup id=fnref:16><a href=#fn:16 class=footnote-ref role=doc-noteref>16</a></sup>。</p><p>また、一般にファイアウォールと呼ばれる機器で実装されるトラフィック制御も「なにかのしくみ」で出来れば嬉しいです。というより、Azure の DC では必須です。Azure では、ユーザーが<strong>ステートフルな L3/L4 ファイアウォール</strong>のルールを柔軟に構成できる NSG と呼ばれる機能を提供しているので <sup id=fnref:17><a href=#fn:17 class=footnote-ref role=doc-noteref>17</a></sup>、「なにかのしくみ」で実現する必要があります。</p><p>さらに、TCP のようなコネクション志向のプロトコルでは、コネクションを確立 (e.g. 3-way handshaking) した後にも同じ VM に対してパケットを運んであげる必要があるので、<strong>L4 レベルでのパーシステンス (セッション維持)</strong> の機能も忘れてはいけません。</p><p>つまり「なにかのしくみ」は、Inbound 通信に対して次のような機能を提供します。</p><ul><li>負荷分散アルゴリズムによる宛先 DIP の決定</li><li>ステートフルな (L3/L4) ファイアウォール</li><li>TCP コネクションのパーシステンス</li></ul><h4 id=outbound-通信>Outbound 通信
<a class=heading-link href=#outbound-%e9%80%9a%e4%bf%a1><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>右側の話です。</p><p>こちらは非常にシンプルで、「なにかのしくみ」がやることは一つだけです。つまり、DIP → VIP に変換する、<strong>ステートフルな Source NAT (SNAT)</strong> 機能です。</p><p>例えば サービス1 の <code>DIP11</code> がインターネットと通信したい場合、「なにかのしくみ」はサービス 1 の VIP <code>VIP1</code> を SNAT 用に使うことを考えます。次に、<code>VIP1</code> で空いているエフェメラル ポートを検索し、最後にそのパケットを DC 外部に放出します。</p><p>また、Outbound 通信への戻り (図中の細い線) は、宛先 IP アドレス VIP 宛になっているので、適切に処理 (今度は DNAT) してあげる必要があります。したがって、SNAT 装置一般に言える話となりますが、「なにかのしくみ」ではどの VM がどの宛先に Outbound の送信をだしたか？という SNAT の履歴を保持しておく必要があります。</p><h4 id=新しいなにかのしくみへ>新しい「なにかのしくみ」へ
<a class=heading-link href=#%e6%96%b0%e3%81%97%e3%81%84%e3%81%aa%e3%81%ab%e3%81%8b%e3%81%ae%e3%81%97%e3%81%8f%e3%81%bf%e3%81%b8><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>負荷分散、パーシステンス、ファイアウォール、SNAT という様々な機能を提供する「なにかのしくみ」ですが、さらに Azure DC の特性に合わせて、次の事項も満たすよう要請されます。</p><ul><li>スケールし、高い可用性を実現できる (N+1 冗長化できる)</li><li>安価に構成するため、専門的な HW (e.g. ASIC, FPGA) を使わずに一般的なサーバーでソフトウェア実装できる</li><li>任意の場所の物理サーバーに、任意のサービスのバックエンド サーバー (VM) をデプロイできる</li><li>テナント (ユーザー) ごとに完全に分離された、論理的なネットワーク環境を提供する</li></ul><p>なかなか難儀な話ですよね・・・。詳しくは論文を見てもらえばと思うのですが、既存の H/W ロードバランサーや SLB ではこのような要件を満たすことができませんでした。そこで Microsoft は Ananta の開発に取り組みました。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/0fb50398-e902-a279-8e64-a71e8bc45bf5.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h2 id=ananta>Ananta
<a class=heading-link href=#ananta><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>前述の「なにかのしくみ」を実現するのが Ananta です。</p><p>負荷分散、パーシステンス、ファイアウォール、SNAT をやりたいんだなぁという気持ちを理解していたら、Ananta の論文も結構すんなり読めます。例えば論文 <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> の Introduction をざっとみるとお気持ちを改めて知ることができます</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/eed8ed36-7027-39a0-2e9c-2d20858369d5.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h3 id=アーキテクチャ>アーキテクチャ
<a class=heading-link href=#%e3%82%a2%e3%83%bc%e3%82%ad%e3%83%86%e3%82%af%e3%83%81%e3%83%a3><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>Ananta は、次の 3 つのコンポーネントで構成されています。</p><ul><li>Ananta Manager</li><li>MUX (Multiplexer)</li><li>Host Agent</li></ul><p>SDN (Software Defined Network) の言葉を使って説明すると、Ananta Manager がコントロール プレーン、MUX と Host Agent がデータ プレーンです。ネットワークに明るくない自分にとっては、SDN に馴染みがなく、当初コントロル/データ プレーンを理解するのに少々時間がかかりました。なので、会社のアナロジーを導入して説明すると、次のようになります。</p><ul><li>Ananta Manager は非常に偉い役職なので、MUX と Host Agent に仕事のやり方について命令するだけで、自分は手を動かしません。</li><li>MUX と Host Agent は下っ端なので、何も考えず、教えられた通りに仕事をします。</li></ul><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/04e34fc9-ef6d-28e2-e30e-814b684e1c6a.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><p>各コンポーネントがどのような役割を果たすか、詳しく見ていきます。</p><h4 id=ananta-manager-am>Ananta Manager (AM)
<a class=heading-link href=#ananta-manager-am><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><ul><li>コントロールプレーンを担うコンポーネント<ul><li>MUX と Host Agent の面倒を見る</li></ul></li><li>AM は DC 内のホスト (VM がデプロイされる物理サーバー) にデプロイされる<ul><li>AM ごとに 5 つのコピー (レプリカ, replica) を作成することで冗長化されている</li></ul></li><li>提供する主な機能:<ul><li>MUX が管理すべきサービスのデータを、MUX に共有する</li><li>SNAT で必要な諸々の管理</li></ul></li></ul><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/be6dca64-491d-cea3-23d8-8291ec96c292.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h4 id=mux-multiplexer>MUX (Multiplexer)
<a class=heading-link href=#mux-multiplexer><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><ul><li>データプレーンの真ん中のティアに存在<ul><li>汎用的な x86 サーバー上でソフトウェア実装されている</li><li>MUX も HA 同様 DC 内部のホスト (物理サーバー) 上にデプロイされている</li></ul></li><li>MUX はチームとして動く<ul><li>このチームを MUX プールと呼ぶ</li><li>典型的には一つの MUX プールに 8 個の MUX が存在 (レプリカではない）</li><li>プール内のすべての MUX は同じ情報を共有する</li><li>AM からチーム全体に対して、チームで担当すべきサービスの情報 (VIP と DIP の対応表、負荷分散の規則、SNAT の情報、&mldr;) が共有される</li></ul></li><li>MUX の主な役割:<ul><li>BGP を喋って、ティア 1 ルーターに自分が担当する VIP のリストを知らせる</li><li>実際にパケットがルーターから届いたときに、ハッシュ計算に基づいて負荷分散先を決める</li><li>負荷分散先を決めたら、HA に届けるために IP-in-IP によるカプセル化を行って送信する</li></ul></li></ul><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/79791dc1-24b7-6932-79ad-9248b93bf80c.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><h4 id=host-agent-ha>Host Agent (HA)
<a class=heading-link href=#host-agent-ha><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><ul><li>データプレーンの一番下のティアに存在</li><li>ハイパーバイザーで動く仮想スイッチ上のソフトウェア</li><li>主な機能:<ul><li>MUX がカプセル化をしたパケットを解く</li><li>宛先が VIP になっているのを DIP になおす (DNAT)</li><li>ACL に基づいてアクセスを拒否するか受け入れるか決める (ファイアウォールとしての役割)</li></ul></li></ul><h3 id=パケットの流れ>パケットの流れ
<a class=heading-link href=#%e3%83%91%e3%82%b1%e3%83%83%e3%83%88%e3%81%ae%e6%b5%81%e3%82%8c><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>Ananta の SIGCOMM 発表スライドからまんま拝借したスライドを使って、Inbound 通信と Outbound 通信のそれぞれで、パケットがどのような度をするのか解説します。</p><h4 id=inbound-通信-1>Inbound 通信
<a class=heading-link href=#inbound-%e9%80%9a%e4%bf%a1-1><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>インターネット等 DC 外のクライアントから、VIP に対してアクセスがあったときのパケット フローです。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/f14761c1-d747-a527-9c6f-85ae1cd4b8fc.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><p>前提: MUX は、ルーターに対して、担当する VIP を経路広報している (ルーターは各 VIP のネクストホップを把握している)。</p><ol><li>パケットが外部から VIP 宛にやってきたとき、ECMP (Equal Cost Multi-Path) プロトコルによって、まず第 1 段階の負荷分散が行われる。つまり、(BGP ピアが張られている MUX のうち) ランダムに選択された MUX のどれかにパケットが転送される。</li><li>MUX はある負荷分散アルゴリズムに基づいて DIP を一つ決定する</li><li>IP-in-IP によってカプセル化されたパケットを、対象の DIP をホストしている物理サーバーの HA に送信する。</li><li>HA はカプセル化を解き (i.e. 外側の IP ヘッダを外し)、DNAT (i.e. 宛先を VIP から DIP に書き換え) を実行。同時に NAT テーブルを更新する。</li><li>HA は このパケット DIP に送信し、VM はパケットを受け取る。</li><li>(ここからは戻りの話) VM が、クライアントの IP アドレス宛のパケットをゲートウェイに送信する</li><li>HA はそれを横取りして、NAT テーブルに基づきリバース NAT を実行 (i.e. ソース IP を DIP から VIP に書き換え)。</li><li>MUX 上で NAT していないことによって、DSR (direct server return) の効果が得られ、DC 内でパケットは MUX を介さずに直接ルーターに運ばれる。そしてクライアントに戻りのパケットが届く。</li></ol><h4 id=outbound-通信-1>Outbound 通信
<a class=heading-link href=#outbound-%e9%80%9a%e4%bf%a1-1><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>VM からインターネット宛の通信をする際のパケット フローです。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/6993cdbe-8509-e5a7-702c-6acf1db1f26a.png alt=image.png><figcaption><p>image.png</p></figcaption></figure><ol><li>VM が「ソース IP が DIP, 宛先が任意のグローバル IP アドレス」のパケットをデフォルトゲートウェイに向けて送信する</li><li>HA はこれを検知して横取り。SNAT が必要なパケットであることを確認してから、「DIP 用の VIP と、空いているエフェメラルポートを教えてくれ～」と AM にお願いする</li><li>AM は VIP とポートを用意し、この旨を DIP を担当する MUX のグループに知らせる</li><li>HA もこの情報 (i.e. VIP とポートの組) を AM から受け取る</li><li>HA が、IP ヘッダのソース IP:ポートを通知された &ldquo;VIP:ポート&rdquo; に変換 (SNAT) する。変換後のパケットは MUX を介さずルーターに直接渡す</li><li>戻りのパケットに関しては、Inbound と同様</li></ol><h3 id=ここがすごいぞ-ananta>ここがすごいぞ Ananta
<a class=heading-link href=#%e3%81%93%e3%81%93%e3%81%8c%e3%81%99%e3%81%94%e3%81%84%e3%81%9e-ananta><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h3><p>ざっくり以上のような流れでパケットを処理するのですが、細かいところに素晴らしい技術が取り入れられています。ここでは、それらを掘り下げて取り上げます。</p><h4 id=fastpath>FastPath
<a class=heading-link href=#fastpath><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>Ananta では、同一 DC 内サービス間の通信を検知した時に無駄のない通信経路に切り替えます。この機能を FastPath と呼んでいます。FastPath
は、<strong>MUX の使用率を大幅に削減する</strong>ことに成功しました。</p><p>同じ DC 内にある 2 つのサービス (e.g. サービス 1, 2) があった時、同 DC 内とは言え、お互いはご近所であることを知りません。したがって、サービス間は VIP 同士でやりとりされます。ただ、最初の通信 (i.e. MUX の負荷分散による宛先 DIP の決定) が済んでしまえば、その後は DIP 同士でも通信が可能です <sup id=fnref:18><a href=#fn:18 class=footnote-ref role=doc-noteref>18</a></sup>。</p><p>「MUX 経由で送信する必要がない通信に対しては、MUX を介さず直接やり取しよう」というのが、FastPath の気持ちです。</p><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/915a6d4d-b9c2-572f-2575-ccb29ded798b.png alt=FastPath width=500><figcaption><p>FastPath</p></figcaption></figure><ol><li><code>DIP1</code> 上の VM から、宛先が <code>VIP2</code> の TCP SYN が送信される。<code>VIP2</code> は <code>MUX2</code> の担当なので、Host Agent は、パケットを <code>MUX2</code> に届ける。</li><li><code>MUX2</code> はパケットを <code>DIP2</code> に転送する<sup id=fnref:19><a href=#fn:19 class=footnote-ref role=doc-noteref>19</a></sup>。</li><li>リプライのパケットは、まず <code>MUX1</code> に返信される。</li><li><code>MUX1</code> は返信パケットを <code>DIP1</code> に転送する。</li><li>TCP コネクションが確立したら、<code>MUX2</code> は <code>MUX1</code> にリダイレクトのメッセージを送信する。</li><li><code>MUX1</code> は <code>DIP2</code> の IP/ポート を Host Agent に通知。</li><li><code>MUX1</code> は <code>DIP1</code> の IP/ポート を Host Agent に通知。</li><li>最終的に、<code>DIP1</code> と <code>DIP2</code> は <code>MUX*</code> を介さず直接やりとりするようになる。</li></ol><h4 id=staged-event-driven-seda>Staged event-driven (SEDA)
<a class=heading-link href=#staged-event-driven-seda><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><figure><img src=https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/127839/e9db44da-9b49-079a-faf2-5d2343d4d892.png alt=SEDA width=500><figcaption><p>SEDA</p></figcaption></figure><p>Ananta Manager はたくさんの責務がありますが、<strong>タスクに優先度をつけて、ワークロードが増えた場合にでもミッションクリティカルなタスクは続行できるような工夫</strong>を実装していています。ベース技術は、SEDA (staged event-driven) <sup id=fnref:20><a href=#fn:20 class=footnote-ref role=doc-noteref>20</a></sup> と呼ばれる、状態遷移を使ったスレッド プールの管理方法です。Ananta では加えて次の 2 点を改良しています。</p><ol><li>異なる状態で同じスレッドプールを共有できる</li><li>各状態で複数の順位付きキューを付与できる</li></ol><h4 id=paxos>Paxos
<a class=heading-link href=#paxos><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>Ananta Manager は冗長化を目的として 5 つの全く同じコピー（レプリカ）が存在しますが、整合性のためメインに選ばれた 1 台のみが実際に動作します。メインの AM を選ぶために、Ananta は<strong>分散システムでの合意形成アルゴリズムである Paxos <sup id=fnref:21><a href=#fn:21 class=footnote-ref role=doc-noteref>21</a></sup> を利用</strong>します。</p><ul><li><a href=https://ja.wikipedia.org/wiki/Paxos%E3%82%A2%E3%83%AB%E3%82%B4%E3%83%AA%E3%82%BA%E3%83%A0 class=external-link target=_blank rel=noopener>Paxosアルゴリズム - Wikipedia</a></li><li><a href=https://qiita.com/kumagi/items/535c9b7a761d2ed52bc0 class=external-link target=_blank rel=noopener>今度こそ絶対あなたに理解させるPaxos - Qiita</a></li><li><a href=http://matope.hatenablog.com/entry/2018/05/13/204749 class=external-link target=_blank rel=noopener>ざっくり理解するPaxos - 小野マトペの納豆ペペロンチーノ日記</a></li><li><a href=https://www.slideshare.net/pfi/paxos-13615514 class=external-link target=_blank rel=noopener>Paxos - SlideShare</a></li></ul><h4 id=ip-in-ip-トンネリング>IP in IP トンネリング
<a class=heading-link href=#ip-in-ip-%e3%83%88%e3%83%b3%e3%83%8d%e3%83%aa%e3%83%b3%e3%82%b0><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>MUX から Host Agent (HA) に送信する際、IP-in-IP (IPIP) トンネリングによって、IP パケットをカプセル化します。この結果、<strong>MUX と HA をネットワーク的に遠い場所に配置できる</strong>ようになります。つまり、物理的ではなく、仮想的な同一 L2 セグメントに MUX と HA がいれば通信できる訳です。</p><p>この技術は HA 間の通信にも適用されます。Azure では、任意のホスト (物理サーバー）に VM がデプロイされるので、同一 L2 セグメントに配置しなくてはならないという制約は非常にボトルネックになってしまいます。</p><h4 id=dsr-direct-server-return>DSR (Direct Server Return)
<a class=heading-link href=#dsr-direct-server-return><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>Inbound 通信の戻りの際、Host Agent は MUX を介さず、直接ルーターにパケットを送信します (&ldquo;Inbound&rdquo; セクションのステップ 8 に相当する)。これは <strong>DSR (Direct Server Return)</strong> の一例となっています。MUX にいちいち戻り通信が帰ってこないので、MUX の負担を軽減することができるのですが、このテクニックを実現する鍵は「Host Agent 上での DNAT」です。</p><p>一般的な NAT の話として、NAT する場合はリバース NAT もしなければ通信が成り立たないため、NAT 装置を無視して戻り通信を送信することはできません。なので、もし MUX 上で DNAT (VIP -> DIP) してしまうと DSR は実現できません。Ananta では、ロードバランシングに不可欠な <strong>DNAT 機能を MUX から切り離し、HA に委任するアーキテクチャによって DSR が実現</strong>されています。</p><h4 id=tcp-コネクションの永続化>TCP コネクションの永続化
<a class=heading-link href=#tcp-%e3%82%b3%e3%83%8d%e3%82%af%e3%82%b7%e3%83%a7%e3%83%b3%e3%81%ae%e6%b0%b8%e7%b6%9a%e5%8c%96><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h4><p>パーシステンスのために、<strong>MUX はコネクションと DIP の対応関係をフロー テーブルに記憶します</strong>。ただ、ティア 1 ルーターが選ぶ MUX は、BGP の ECMP を使っている限り、ランダムに決定されます。そのため、フロー テーブルを MUX プールで共有することで、どの MUX にトラフィックが来ても TCP コネクションを張っている DIP に割り振ることができます。</p><p>冒頭にも紹介したように、Azure Load Balancer はバックエンド プールに新しいサーバーを追加しても、既存のセッションに関しては新しいサーバーに振り分けられることはありません。この動作は、<strong>バックエンド プールからの VM の切り離しでは、MUX のフロー テーブルがリセットされない</strong>ことに起因していますが、「バックエンド プールからの切り離し = Ananta Manager が MUX Pool に割り振る情報の変更」なので、これが MUX pool で共有している現在のフロー テーブルと関係ないことからも納得の動作とわかります。</p><p>ちなみに、フロー テーブルをリセットするには、VM の再配置が必要となります (再配置を伴う特別な VM の再起動）。</p><h2 id=まとめ>まとめ
<a class=heading-link href=#%e3%81%be%e3%81%a8%e3%82%81><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><p>この記事では、Ananta が Azure の特性の合わせて作られた L4 のロードバランサー & SNAT 装置であることをみてきました。</p><p>ロードバランサーとしての Ananta は、MUX と呼ばれる装置が主だって負荷分散しています。従来の H/W LB では 1+1 冗長化しか達成できませんでしたが、Ananta は MUX プールに存在する複数 MUX を Ananta Manager によってコントロールすることで N+1 冗長化を実現しました。MUX はデーターセンターの任意のホストにデプロイされるために、結果として、MUX の可用性と DC の可用性はほぼ等しくなります。そして、MUX をコントロールする Ananta Manager もまた、Paxos や SEDA といった技術でホットスタンバイのレプリカが作成できるため、データプレーン・コントロールプレーンともに高度な冗長化が実装されています。</p><p>また、Host Agent でも、多くのロードバランシング タスク (e.g. 負荷分散の DNAT, ステートフル L4 ファイアウォール) のオフロード、FastPath、DSR といった技術によって、MUX の負担を減らしていることを紹介しました。代わりに Host Agent での処理負担が増えてしまいますが、それは論文の Figure 11 の通り、MUX への負荷集中を考慮すればかなり限定的なものです。寧ろ、多くの処理を Host Agent 側にオフロードしたことによって、実装の最適化がしやすくなりました。実際に、2017 年に提案された VFP (virtual flitering platform) <sup id=fnref1:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> では、Open vSwitch に着想を得てスイッチをコントロール/データ プレーンに分解し、処理の高速化・スケールを可能としました。また、VFP の処理の一部をハードウェア実装した SmartNIC<sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> が 2018 年にも提案されています。</p><p>2019 年 12 月現在、Ananta はもはや進化に進化を重ねてしまい、そのままの形では Azure の DC で使われていませんが、そのアーキテクチャの血は依然受け継がれています。冒頭で示したように、進化の一部は公開技術として提供されていますが、まだ非公開のものも多くあります。Azure のロードバランサー/データーセンターに関する新たな技術資料が公開され、そしてそれを皆さんに共有する機会が来ることを、いち Azure ファンとして楽しみにしています。</p><h2 id=参考文献>参考文献
<a class=heading-link href=#%e5%8f%82%e8%80%83%e6%96%87%e7%8c%ae><i class="fa-solid fa-link" aria-hidden=true title=見出しへのリンク></i>
<span class=sr-only>見出しへのリンク</span></a></h2><ul><li><a href=https://www.slideserve.com/noura/ananta-cloud-scale-load-balancing class=external-link target=_blank rel=noopener>PPT - Ananta: Cloud Scale Load Balancing PowerPoint Presentation, free download - ID:1614411</a></li><li><a href=http://yunazuno.hatenablog.com/entry/2016/02/29/090001 class=external-link target=_blank rel=noopener>ロードバランサのアーキテクチャいろいろ - yunazuno.log</a></li><li><a href=https://www.slideshare.net/ShuheiUda/azure-networking-165852712 class=external-link target=_blank rel=noopener>サポート エンジニアが Azure Networking をじっくりたっぷり語りつくす会</a> (p.37&ndash;)</li><li><a href=https://channel9.msdn.com/Events/de-code/2016/INF-001 class=external-link target=_blank rel=noopener>詳説 Azure IaaS ～私はインフラが好きだ～ | de:code 2016 | Channel 9</a> (50 分過ぎ～)</li><li><a href=https://www.infraexpert.com/study/loadbalancer7.html class=external-link target=_blank rel=noopener>ロードバランサ - 基本構成、IPアドレス割り当て、アドレス変換、用語説明</a></li><li><a href=https://blog.envoyproxy.io/introduction-to-modern-network-load-balancing-and-proxying-a57f6ff80236 class=external-link target=_blank rel=noopener>Introduction to modern network load balancing and proxying</a></li><li><a href=https://www.usenix.org/conference/srecon15/program/presentation/shuff class=external-link target=_blank rel=noopener>Building a Billion User Load Balancer | USENIX</a></li><li><a href=https://www.janog.gr.jp/meeting/janog40/application/files/3415/0208/4443/janog40-sp6lb-kanemaru-03.pdf class=external-link target=_blank rel=noopener>自作ロードバランサ開発 (pdf)</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Patel, Parveen, et al. &ldquo;Ananta: Cloud scale load balancing.&rdquo; ACM SIGCOMM Computer Communication Review. Vol. 43. No. 4. ACM, 2013. <a href=https://conferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p207.pdf class=external-link target=_blank rel=noopener>https://conferences.sigcomm.org/sigcomm/2013/papers/sigcomm/p207.pdf</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>今回紹介しきれなかったこれらの論文も大変興味深いので、いつか紹介したいと思っています。&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Gandhi, Rohan, et al. &ldquo;Duet: Cloud scale load balancing with hardware and software.&rdquo; ACM SIGCOMM Computer Communication Review. Vol. 44. No. 4. ACM, 2014.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Gandhi, Rohan, et al. &ldquo;Rubik: unlocking the power of locality and end-point flexibility in cloud scale load balancing.&rdquo; 2015 {USENIX} Annual Technical Conference ({USENIX}{ATC} 15). 2015.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>Firestone, Daniel. &ldquo;VFP: A Virtual Switch Platform for Host SDN in the Public Cloud.&rdquo; 14th USENIX Symposium on Networked Systems Design and Implementation (NSDI 17). 2017. <a href=https://www.microsoft.com/en-us/research/publication/vfp-virtual-switch-platform-host-sdn-public-cloud/ class=external-link target=_blank rel=noopener>VFP: A Virtual Switch Platform for Host SDN in the Public Cloud - Microsoft Research</a>&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Firestone, Daniel, et al. &ldquo;Azure accelerated networking: SmartNICs in the public cloud.&rdquo; 15th USENIX Symposium on Networked Systems Design and Implementation (NSDI 18). 2018.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>SmartNIC に関しては、次のような資料も理解の役に立ちます。&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p><a href="https://www.microsoft.com/en-us/research/research-area/systems-and-networking/?facet%5Btax%5D%5Bmsr-research-area%5D%5B0%5D=13547&amp;sort_by=most-recent" class=external-link target=_blank rel=noopener>Systems & networking - Microsoft Research</a> にネットワーク関連の技術が公開されています。&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>Docker に慣れている人は、<code>docker-compose.yml</code> で指定する <code>services:</code> を想像するとわかりやすいかもしれません。Docker でのサービスの説明は &ldquo;<a href=http://docs.docker.jp/get-started/part3.html#id4 class=external-link target=_blank rel=noopener>Part 3：サービス — Docker-docs-ja 17.06.Beta ドキュメント</a>&rdquo; にあります。&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>最近では、たくさんの小さなサービスで一つのアプリケーションを構成する設計思想 &ldquo;マイクロサービス アーキテクチャ&rdquo; が有名ですね。同じ Web アプリケーションでも、アプリケーション全体を一つのサービスで構成するのか、それとも複数のサービスを組み合わせるのかといったように、サービスの粒度は人間の裁量次第です。&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>なぜ VIP には「仮想」という名前がついているかと言うと、特定のネットワーク インターフェース (NIC) に関連付けられた &ldquo;物理的な&rdquo; IP アドレスではないからです。&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>LB のバックエンドに居るホストのことを <strong>実サーバー (real server)</strong>、あるいはバックエンド サーバー等と言いますが、Azure では実サーバーは「仮想マシン」である点に注意してください (本記事では、ややこしいので実サーバーという呼称は使いません)。&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Azure は雲ではありません…&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p><a href=https://research.google/pubs/pub43837/ class=external-link target=_blank rel=noopener>Jupiter Rising: A Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network – Google Research</a>&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:15><p>SLA を満たす最適な資源割り当てに関する手法. <a href=https://www.microsoft.com/en-us/research/publication/dynamic-resource-allocation-in-the-cloud-with-near-optimal-efficiency/ class=external-link target=_blank rel=noopener>Dynamic Resource Allocation in the Cloud with Near-Optimal Efficiency - Microsoft Research</a>&#160;<a href=#fnref:15 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:16><p>また、受け取る VM (e.g. `DIP11) 側の気持ちになってみると、宛先 IP アドレス が VIP (e.g. VIP1) のままではパケットを受け取ったときにびっくりしてしまいます。なので負荷分散先の VM が決まったら、宛先を DIP に変換しなければなりません。Destination IP Address を NAT するので、この処理は DNAT と呼ばれます。&#160;<a href=#fnref:16 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:17><p>Azure ではネットワークセキュリティグループ (network security group; NSG)、AWS ではセキュリティーグループ、GCP ではファイアウォールルールと呼ばれる機能で提供される、L4 のステートフル ファイアウォールです。&#160;<a href=#fnref:17 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:18><p>DIP は VM 内部で有効な IP アドレス (いわゆる仮想ネットワークのアドレス) なので、DC 内部での物理 L3 ネットワークで有効ではありません。DIP 同士の通信でも、依然 IP-in-IP カプセル化によって L2 セグメントを越える必要があります。&#160;<a href=#fnref:18 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:19><p>ステップ 2. について、この時 Host Agent から送られたパケットの宛先 IP アドレスはあくまで <code>DIP2</code> であり <code>VIP2</code> でははないので、インターネットからの通信とはちょっと動作が違う点に注意。&#160;<a href=#fnref:19 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:20><p>M. Welsh, D. Culler, and E. Brewer. SEDA: An Architecture for Well-Conditioned, Scalable Internet Services. In SOSP, 2001.&#160;<a href=#fnref:20 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:21><p>L. Lamport. The Part-Time Parliament. ACM TOCS, 16(2):133–169, May 1998&#160;<a href=#fnref:21 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer></footer></article></section></div><footer class=footer><section class=container>©
2024 -
2025
Junya Yamaguchi (@openjny)
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script><script defer src=https://static.cloudflareinsights.com/beacon.min.js data-cf-beacon='{"token": "0a99e4ebb1e342beb03db419a6eea4a6"}'></script></body></html>